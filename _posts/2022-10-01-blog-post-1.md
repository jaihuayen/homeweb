---
title: 'RegNet: Regular Network'
date: 2022-10-01
permalink: /posts/2022/10/blog-post-2/
tags:
  - Network Search
  - AI
---

過往的尋求好模型的方法，即是NAS(Neural Architecture Search)，但是過往的NAS技術是在一個給定設計空間(designed search space)，並在此空間裡搜索出最佳的一組參數。[這篇研究](https://arxiv.org/pdf/2003.13678.pdf)的重點是在於如何設計出設計空間(design design space)，而不僅是搜索出最佳的一組參數。

---
## Chapter 1: Introduction
過往的研究如LeNet、AlexNet、VGG和ResNet等，皆讓我們對於模型的設計有更好的概念，像是模型的卷積、資料大小、模型深度、模型殘差等。而NAS為一種從一個設計空間中找出一個好的模型，但此方法無法讓我們知道如何找到一個好的網路架構。

過往的模型搜尋方法中，以手工設計(manual design)以及NAS為主。而各自的好處如下：
1. 手工設計的好處：可解釋性、簡單、泛化
2. NAS的好處：半自動化達到我們要求模型的效果

而這篇研究是期望可以達到以上兩種方法的優點，並實作在VGG或是ResNet等經典模型，並從一個設計空間AnyNet，經過一些方法得到我們要的設計空間RegNet。

先講此研究的結論：
1. RegNet在執行效率上非常好
2. 比起EfficientNet，效果更好且訓練速度在GPU上快了5倍之多

## Chapter 2: Design Space Design
下圖展示了此篇研究的方法，比起找到唯一最好的模型，在此研究模型參數的母體，以此研究出一個可泛化的模型設計準則：

![](/homeweb/images/regnet/regnet-1.png)

在此我們藉由從特定的設計空間中，抽樣一些參數組合，找到此設計空間的模型分佈，並透過統計數據進行分析此設計空間的性質。藉由這些步驟，找到更簡單、效果更好的模型。並且從模型的偏誤分佈進行分析後，我們可以獲得更多且更穩健的資訊。

目標希望為：
1. 簡化模型的複雜度
2. 增加設計空間的可解釋性
3. 優化並且維持設計空間的品質
4. 在設計空間中維持模型的多樣性

具體的步驟如下：
1. 先從設計空間抽樣一些模型
2. 畫出error empirical distribution function (EDF)並用empirical bootstrap來獲得更多深入觀察
3. 重新規劃出新的設計空間

在此隨機挑選了500個模型，並各自跑了10個epochs。接下來就是發現一些準則，建構出一個個設計空間，來一步步挑選出好的模型。

### 2.1 AnyNetXA
與許多的深度學習模型框架類似，由stem、body以及head組成：

![](/homeweb/images/regnet/regnet-2.png)

* stem：輸入資料層
* body：模型的主架構層，用來提取數據特徵，裡面再由許多stage組成(大部分的論文以block稱呼他)，每一個stage再由block組成(大部分的論文以layer稱呼他)
* head：模型的輸出層，依照不同的任務類型來調整輸出內容

模型的優化，全部都在body中進行，總共有4個stage，裡面的超參數空間如下：

1. block的層數 $d_{i}$：滿足 $1\leq d_{i} \leq 16$
2. 每一層的通道數 $w_{j}$：滿足 $8 \times k$，$1\leq k \leq 128$
3. bottleneck ratio (EfficientNet中的瓶頸率)：$b_{i} \in \{1,2,4 \}$
4. 分組卷積的組數 (平行的layer數)：$g_{i} \in \{1,2,...,32 \}$

總共有$(16 \cdot 128 \cdot 3 \cdot 6)^4 \approx 10^{18}$ 可能的模型選擇在AnyNetX中。

### 2.2 AnyNetXB
固定bottleneck ratio $b_{i}$後，發現error EDF幾乎沒有變化：

![](/homeweb/images/regnet/regnet-3.png)

而整個設計空間的樣本空間量級卻減少很多：$(16 \cdot 128 \cdot 6)^4 \times 3 \approx 6.8 \cdot 10^{16}$

### 2.3 AnyNetXC
共享分組卷積的組數$g_{i}$後，發現error EDF幾乎沒有變化：

![](/homeweb/images/regnet/regnet-4.png)


而分組卷積的組數有6種：$(16 \cdot 128 )^4 \times 3 \times 6 \approx 3.2 \cdot 10^{14}$

### 2.4 AnyNetXD
當我們使用遞增的模型寬度$w_{i}$時，發現設計空間內的模型表現越來越好：

![](/homeweb/images/regnet/regnet-5.png)

此時的模型樣本空間量級為：$\frac{(16 \cdot 128 )^4 \times 3 \times 6}{4!} \approx 1.3 \cdot 10^{12}$ 

### 2.5 AnyNetXE
當我們使用遞增的模型層數$d_{i}$時，發現設計空間內的模型表現越來越好：

![](/homeweb/images/regnet/regnet-6.png)

此時的模型樣本空間量級為：$\frac{(16 \cdot 128 )^4 \times 3 \times 6}{(4!)^2} \approx 5.5 \cdot 10^{11}$ 

整個過程將模型的樣本空間量級減少了$O(10^7)$。再次總結上面的結果：
1. bottleneck ratio 共享
2. 分組層數共享
3. 模型寬度增加
4. 模型層數增加

### 2.6 RegNet

從上一節發現，模型的寬度增加，對於模型的表現有正向的效果。接下來便是需要研究，模型的寬度要以什麼樣的方式增加，才會更好。模型的深度也是此部分的研究目的。

下圖為從AnyNetE的空間中，抽取20個最優的模型隨著模型寬度的增加，模型層數增加的折線圖：

![](/homeweb/images/regnet/regnet-7.png)

最後近似出一個函數為：

$$u_{j} = w_{0} + w_{a} \cdot j \quad \textrm{for} \quad 0 \leq j \lt d$$

其中$d$為模型深度，$w_{0} > 0$為初始模型寬度，$w_{a}>0$為模型斜率。

但是實際上，在操作的時候，此篇研究採取此做法：

$$w_{i}=w_{0} \cdot w_{m}^{Round(s_{j})}$$
$$d_{i}=\sum_{j}1[Round(s_{j})=i]$$

其中$d<64$，$w_{0}, w_{a}<256$, $1.5 \leq w_{m} \leq 3$，這便是RegNet的樣本空間了！此空間的量級為$3.0 \times 10^8$。EDF曲線如下圖，並且RegNet再用Grid Search進行優化：

![](/homeweb/images/regnet/regnet-8.png)

其中還有許多細節，在這邊先不贅述，有興趣的讀者可以去看原文。

## Chapter 3: Result
與ResNet相比，模型的表現更好：
![](/homeweb/images/regnet/regnet-9.png)

與EfficientNet相比，模型效果更好，並且在inference的速度有5倍之快：

![](/homeweb/images/regnet/regnet-10.png)

## Chapter 4: Conclusion
此篇研究最大的不同，在於不同於NAS的搜尋方式，搜索出一個最佳的模型，而是藉由了解設計空間，設計出一套程序，去找尋到較好的設計空間，並從中找到較佳的模型。結果來看，相較於ResNet以及EfficientNet，效果更好，並且inference速度更快，藉此達到更可以實務上操作的AutoML，並保持模型輕量化。

## Reference
1. Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., & Dollár, P. (2020). Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10428-10436).
2. Radosavovic, I., Johnson, J., Xie, S., Lo, W. Y., & Dollár, P. (2019). On network design spaces for visual recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1882-1890).